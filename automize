#! /bin/bash

# Automize everything!
# 1. Set cut info: pass arguments into command line
# 2. Event selection -> access data in MG5 -> output: .root file
# 3. .root file copied & moved to datafiles
# 4. .root file in datafiles opened and histograms normalized
# 5. Create .xml config files
# 6. hist2workspace .xml files
# 7. Resulting workspace file (.root) opened and statistical analysis performed

# Save this directory, since it'll return to it later
thisdir=$(pwd)

####### Step 1 #######
echo "-----Step 1-----"  # This makes it easier to fix errors
# User input
echo "Type run_start run_end for your signal"
read v1 v2
echo "Type cut information: BTag nonBJet MWLow MWHigh MTLow MTHigh MET"
read v3 v4 v5 v6 v7 v8 v9

cutid=$v3$v4$v5$v6$v7$v8$v9

####### Step 2 #######
echo "-----Step 2-----"
# Run event selection
data_directory="/home/yksns23/MG5_aMC_v2_6_3_2/RESEARCH"
cp event_selection/event_selection -t $data_directory
cd $data_directory
python event_selection signal $v1 $v2 1 $v3 $v4 $v5 $v6 $v7 $v8 $v9
python event_selection background_tbZ 1 1 1 $v3 $v4 $v5 $v6 $v7 $v8 $v9
python event_selection background_ttxZ_4j2b2vl 1 1 1 $v3 $v4 $v5 $v6 $v7 $v8 $v9
python event_selection background_ttxZ_lvl2j2b2vl 1 1 1 $v3 $v4 $v5 $v6 $v7 $v8 $v9

####### Step 3 #######
echo "-----Step 3-----"
# Save the .root file both to MG5/selection_results and datafiles
datafile="data_$cutid.root"
find . -maxdepth 1 -type f -name $datafile -exec cp -t selection_results {} \; -exec mv -t $thisdir/create_workspace/datafiles {} \;

cd $thisdir

####### Step 4 #######
echo "-----Step 4-----"
# Open the .root file in datafiles and normalize histograms
# (it will also normalize the background)
./normhist create_workspace/datafiles/$datafile $cutid $v1 $v2

####### Step 5 #######
echo "-----Step 5-----"

# Create channels
source create_channel
for (( run=((v1)); run<=((v2)); run++ )); do
  create_channel $run $datafile $cutid
done

# Create top-level config
source create_top_config
create_top_config $cutid

####### Step 6 #######
echo "-----Step 6-----"
# Run hist2workspace
hist2workspace create_workspace/config/$cutid/wpzp_config_$cutid.xml

####### Step 7 #######
echo "-----Step 7-----"
# Open workspace files and perform statistical analysis
declare -a masses=(300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900)
for (( run=((v1)); run<=((v2)); run++ )); do
  mwp=${masses[(($run-1))]}
  infile="create_workspace/results/data_cut$cutid/wpzp_run${run}_mwp${mwp}_mwp_model.root"
  outfile="sigma_results/signif_cut$cutid.root"
  ./sigma $infile $mwp "run${run}_mwp${mwp}" $outfile
done



